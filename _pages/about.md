---
permalink: /
title: "Samantha Chiu"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Hi there! I'm Samantha (she/her/hers), a graduate student at the University of Iowa studying cognitive neuroscience, psycholinguistics, and sociolinguistics. I'm passionate about using data analysis to improve the human experience. My dual expertise in data science and user experience research (UXR) gives me a unique perspective. I understand how people think and learn, and I use data to understand how they interact with technology. This combination allows me to bridge the gap between raw data and meaningful user experiences, ensuring that solutions are both data-driven and user-centered.

Projects 
====== 
[Uncovering Strategies in Music and Language Learning in Adults](https://samanthalchiu.github.io/posts/2023/02/language-learning/)
------
Highlights:
* R, ggplot
* t-tests, logistic regression
* A/B testing

Feed Me Seymour! A Content Based [Recipe Recommendation System](https://github.com/samanthalchiu/feed-me-seymour/)
------
In an attempt to spice up my meals, I've created a recipe recommendation system that uses similarity between ingredients to recommend new recipes. I learned to data scrape and clean text data and take my first steps into NLP!

Assessing Speech Perception Efficiency from EEG using Machine Learning (Support Vector Machines)
------
To perceive speech, one must be both accurate and efficient to understand each word in the little time it takes to produce it. Here, we focus on extracting how efficient listeners are in speech perception by extracting the time course of word recognition. This project combines electroencephalography (EEG) and machine learning (support vector machine) to decode word recognition over time. This methodology has been successful in decoding word recognition in normal hearing adults within subject and in both high and low impedance EEG systems. Currently, we are 1) validating this method against existing methods and 2) extending this to populations with hearing loss. Collaborators on this project include [McCall Sarrett](https://www.mccallesarrett.com/), [Alexis Black](https://languageanddevelopment.ca/), and [Dick Aslin](https://llamblab.haskins.yale.edu/our-team/).

Effect of Linguistic Environment on Speech Perception
------
Each person has their own unique pronunciation of every word due to someone’s accent, the languages they speak, or some unique individual variation. Therefore, you must adapt to each person’s unique productions to understand them. But does the number of people you adapt to change the way you perceive speech entirely? Or who you speak to? This project examines how one’s auditory linguistic environment (i.e. one’s social network) influence how they perceive speech. Key factors we assess is the number of people listeners speak to each week and how diverse these networks are (i.e. age, gender, language background). This project is supported by the wonderful [Dr. Ethan Kutlu](https://www.ethankutlu.com/).

A brief methods [report](https://samanthalchiu.github.io/publication/2022-11-24-L2VAS) details the motivation and initial investigation into this question. Materials for this project are located [here](https://samanthalchiu.github.io/portfolio/gorilla-clean/) and [here](https://samanthalchiu.github.io/portfolio/redcap-clean/). 

Effect of Psychoacoustic Ability on Speech Perception
------
Speech perception requires some level of adequate input in order to perceive it. Listening to speech in challenging conditions such as listening to speech in noise, for example, is more difficult to perceive. But how does an individual’s ability to perceive the input (i.e. lower or higher ability to discriminate subtle differences) change speech perception? This project assesses whether the ability to hear subtle differences can explain differences in speech perception behavior. Many thanks to [Dr. Sarah Colby](https://psychology.uiowa.edu/maclab/members) for her support on this project. 

Talker Adaptation in Unsupervised and Supervised Learning 
------
Each person has their own unique pronunciation of every word due to someone’s accent, the languages they speak, or some unique individual variation. Therefore, for each person you meet, you must adapt to each person’s unique productions to understand them. Prior work has tracked adaption through phonemic retuning in supervised learning paradigms; however, this does not reflect listening in real world scenarios (i.e. in real time conversation). This project examines whether phonemic retuning can occur in unsupervised learning in a distributional learning paradigm. We find that listeners are indeed able to retune phonemic categories with passive exposure, but do not retain this information for long. This project was presented at the 2022 Graduate Symposium. 

Current results were presented at the [183rd Meeting of the Acoustical Society of America](https://samanthalchiu.github.io/talks/2022-12-07-TalkerShift). Materials for this project are located [here](https://samanthalchiu.github.io/portfolio/mixedmodels/) and [here](https://samanthalchiu.github.io/portfolio/plots/). Currently, I am working on simulating this adaptation in a Bayesian model; materials for this are located [here](https://samanthalchiu.github.io/portfolio/bayesianmodels/).

forked from [academicpages](https://academicpages.github.io) 