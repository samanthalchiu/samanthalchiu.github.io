---
permalink: /
title: "Samantha Chiu"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Hi there! Thanks for stopping by! My name is Samantha (she/her/hers). I'm an NSF Graduate Research Fellow and incoming 4th year graduate student at the University of Iowa. I work with Dr. Bob McMurray in the [MACLAB](https://psychology.uiowa.edu/maclab) and Dr. Ethan Kutlu in the [VoiceLab](https://voice.lab.uiowa.edu/) in the Cognition and Perception within the [Psychological and Brain Sciences](https://psychology.uiowa.edu/) department. My research lines integrate psycholinguistics, sociolinguistics, and cognitive neuroscience and broadly explore how different external factors (i.e. one’s social network) and internal factors (i.e. individual differences in hearing ability) influence speech processing. Understanding what factors contribute to the variability of listeners with diverse speech processing abilities allows us to pinpoint areas of improvement to create accessible and equitable spaces. 

Research Projects: Active 
====== 

Assessing Speech Perception Efficiency from EEG using Machine Learning (Support Vector Machines)
------
To perceive speech, one must be both accurate and efficient to understand each word in the little time it takes to produce it. Here, we focus on extracting how efficient listeners are in speech perception by extracting the time course of word recognition. This project combines electroencephalography (EEG) and machine learning (support vector machine) to decode word recognition over time. This methodology has been successful in decoding word recognition in normal hearing adults within subject and in both high and low impedance EEG systems. Currently, we are 1) validating this method against existing methods and 2) extending this to populations with hearing loss. Collaborators on this project include [McCall Sarrett](https://www.mccallesarrett.com/), [Alexis Black](https://languageanddevelopment.ca/), and [Dick Aslin](https://llamblab.haskins.yale.edu/our-team/).

Effect of Linguistic Environment on Speech Perception
------
Each person has their own unique pronunciation of every word due to someone’s accent, the languages they speak, or some unique individual variation. Therefore, you must adapt to each person’s unique productions to understand them. But does the number of people you adapt to change the way you perceive speech entirely? Or who you speak to? This project examines how one’s auditory linguistic environment (i.e. one’s social network) influence how they perceive speech. Key factors we assess is the number of people listeners speak to each week and how diverse these networks are (i.e. age, gender, language background). This project is supported by the wonderful [Dr. Ethan Kutlu](https://www.ethankutlu.com/).

A brief methods [report](https://samanthalchiu.github.io/publication/2022-11-24-L2VAS) details the motivation and initial investigation into this question. Materials for this project are located [here](https://samanthalchiu.github.io/portfolio/gorilla-clean/) and [here](https://samanthalchiu.github.io/portfolio/redcap-clean/). 

Effect of Psychoacoustic Ability on Speech Perception
------
Speech perception requires some level of adequate input in order to perceive it. Listening to speech in challenging conditions such as listening to speech in noise, for example, is more difficult to perceive. But how does an individual’s ability to perceive the input (i.e. lower or higher ability to discriminate subtle differences) change speech perception? This project assesses whether the ability to hear subtle differences can explain differences in speech perception behavior. Many thanks to [Dr. Sarah Colby](https://psychology.uiowa.edu/maclab/members) for her support on this project. 

Talker Adaptation in Unsupervised and Supervised Learning 
------
Each person has their own unique pronunciation of every word due to someone’s accent, the languages they speak, or some unique individual variation. Therefore, for each person you meet, you must adapt to each person’s unique productions to understand them. Prior work has tracked adaption through phonemic retuning in supervised learning paradigms; however, this does not reflect listening in real world scenarios (i.e. in real time conversation). This project examines whether phonemic retuning can occur in unsupervised learning in a distributional learning paradigm. We find that listeners are indeed able to retune phonemic categories with passive exposure, but do not retain this information for long. This project was presented at the 2022 Graduate Symposium. 

Current results were presented at the [183rd Meeting of the Acoustical Society of America](https://samanthalchiu.github.io/talks/2022-12-07-TalkerShift). Materials for this project are located [here](https://samanthalchiu.github.io/portfolio/mixedmodels/) and [here](https://samanthalchiu.github.io/portfolio/plots/). Currently, I am working on simulating this adaptation in a Bayesian model; materials for this are located [here](https://samanthalchiu.github.io/portfolio/bayesianmodels/).

Personal Projects
======
Feed Me Seymour! A Content Based [Recipe Recommendation System](https://github.com/samanthalchiu/feed-me-seymour/)
------
In an attempt to spice up my meals, I've created a recipe recommendation system that uses similarity between ingredients to recommend new recipes. I learned to data scrape and clean text data and take my first steps into NLP!  

Research Projects: Dormant
======
No one disputes that science is intended to push forth new answers to new (or old) questions. In that pursuit, some answers (or questions) are published, but are unable to be replicated. In an effort to fight the replication crisis (or at least not contribute to it), these projects are intentionally discontinued. Here lies these projects and the effort I put in to pursue them:

The Combined Effect of Unsupervised and Supervised Learning in Speech Category Acquisition (2018-2021)
------
Humans start developing phonetic categories from infancy and continue to develop these categories well into adulthood. The learning mechanisms in each stage of life change given the environment. Infants learn phonetic categories through passive exposure to the input while children acquire phonetic categories with feedback (i.e. from parents, in the classroom). This project studied the effect of unsupervised learning on supervised learning in acquiring phonetic categories as a parallel to the development of speech categories from infancy to childhood. The results of the first experiment were presented at the [Psychonomic Society 2020 Annual Meeting](https://samanthalchiu.github.io/talks/2020-11-21-audcat).

forked from [academicpages](https://academicpages.github.io) 